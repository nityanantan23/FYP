{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_cleaned_FYP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NPC cancer recurrece Detection with ML\n",
        "> we will train the model of NPC cancer recurrece detection.logistic regression and random forest will be used.\n",
        "\n",
        "- author: Nityanantan\n",
        "- categories: [Python, Machine_Learning]\n",
        "\n"
      ],
      "metadata": {
        "id": "Ko9SxyEwJ0cV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import library"
      ],
      "metadata": {
        "id": "t4QkbbB6J-QY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azkyiep3Jtt8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from google.colab import files\n",
        "from math import isnan\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data and explore"
      ],
      "metadata": {
        "id": "DdYFRpSmKIjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Data_combined.csv')"
      ],
      "metadata": {
        "id": "hQAa7U_FKKGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "ORiZX7A4KK2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "xaap10ydKPdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.astype('object').describe().transpose()"
      ],
      "metadata": {
        "id": "MjLDgx1uKRJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning"
      ],
      "metadata": {
        "id": "YNVhcFl7KUWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.qxmd.com/calculate/calculator_620/nasopharyngeal-cancer-tnm-staging\n",
        "#tnm staging level for NPC\n",
        "\n",
        "def tnm(T,N,M,stage):\n",
        "\tif ((T == \"2\")and(N == \"0\")and(M == \"0\")) :\n",
        "\t\treturn \"0\"\n",
        "\telif ((T == \"1\")and(N == \"0\")and(M == \"0\")) :\n",
        "\t\treturn \"I\"\n",
        "\telif ((T == \"1\") or (T == \"0\")and(N == \"1\")and(M == \"0\")):\n",
        "\t\treturn \"II\"\n",
        "\telif ((T == \"2\")and(N == \"0\")and(M == \"0\")) :\n",
        "\t\treturn \"II\"\n",
        "\telif ((T == \"2\")and(N == \"1\")and(M == \"0\")) :\n",
        "\t\treturn \"II\"\n",
        "\telif ((T == \"1\") or (T == \"0\")and(N == \"2\")and(M == \"0\")):\n",
        "\t\treturn \"III\"\n",
        "\telif ((T == \"2\")and(N == \"2\")and(M == \"0\")):\n",
        "\t\treturn \"III\"\n",
        "\telif ((T == \"3\")and((N == \"0\") or (N == \"1\") or (N == \"2\"))and(M == \"0\")):\n",
        "\t\treturn \"III\"\n",
        "\telif ((T == \"4\")and((N == \"0\") or (N == \"1\") or (N == \"2\"))and(M == \"0\")):\n",
        "\t\treturn \"IVA\"\n",
        "\telif (((T == \"0\") or (T == \"0\") or (T == \"2\") or (T == \"1\") or (T == \"2\") or (T == \"3\") or (T == \"4\"))and(N == \"3\")and(M == \"0\")):\n",
        "\t\treturn \"IVA\"\n",
        "\telif (((T == \"0\") or (T == \"0\") or (T == \"2\") or (T == \"1\") or (T == \"2\") or (T == \"3\") or (T == \"4\"))and((N == \"0\") or (N == \"0\") or (N == \"1\") or (N == \"2\") or (N == \"3\"))and(M == \"1\")):\n",
        "\t\treturn \"IVB\"\n",
        "\treturn stage"
      ],
      "metadata": {
        "id": "R6MG-WiyKWJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean tnm stage unknown\n",
        "df['Stage'] = df.apply(lambda row: np.nan if row['Stage'] == 'unknown' else row['Stage'],axis=1)"
      ],
      "metadata": {
        "id": "wIyMEm22KZIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean tnm stage\n",
        "df['Stage'] = df.apply(lambda row: tnm(row['T'],row['N'],row['M'],row['Stage']) if row['Stage'] is np.nan else row['Stage'],axis=1)"
      ],
      "metadata": {
        "id": "32ZLo1TYKcgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing value analysis\n",
        "\n",
        "mis_val = df.isnull().sum()\n",
        "# Percentage of missing values\n",
        "mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "# Make a table with the results\n",
        "mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "# Rename the columns\n",
        "mis_val_table_ren_columns = mis_val_table.rename(\n",
        "columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "        \n",
        "# Sort the table by percentage of missing descending\n",
        "mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "    '% of Total Values', ascending=False).round(1)\n",
        "        \n",
        "# Print some summary information\n",
        "print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"  +\n",
        "       \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "       \" columns that have missing values.\")\n",
        "# Missing values for training data\n",
        "mis_val_table_ren_columns[:20].style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "BMFxacXOKeoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create target variable from treatement outcome\n",
        "def categorise(row):  \n",
        "    if row['TreatmentOutcome'] == 'Recurrence':\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "df['Recurrence'] = df.apply(lambda row: categorise(row), axis=1)\n"
      ],
      "metadata": {
        "id": "NmB9poGPKgPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop useless feature\n",
        "df=df.drop(['Age_At_Last_Status_Update','Status','FirstPositiveBiopsyDate','Hospital','RT_EndDate','Days_RTEnd_untill_Death','Days_Diag_untill_Death','DeathDate','CT_EndDate','CT_Started_After','DOB','StatusDate','ENT_1st_Visit','Onco_1st_Visit','Critical_Level','T','N','M','TreatmentOutcome','Ethnicity','Occupation','PatientID','Name'], axis=1)"
      ],
      "metadata": {
        "id": "N-_7GxPCKho0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop high null values column\n",
        "df=df.drop(['SurgeriesCount'], axis=1)"
      ],
      "metadata": {
        "id": "p16pxvCBKiqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imputing values in null space\n",
        "\n",
        "def lin_regression_impute(df, model):\n",
        "        # function for predicting missing values with linear regression\n",
        "        cols_num = df.select_dtypes(include=np.number).columns\n",
        "        mapping = dict()\n",
        "        for feature in df.columns:\n",
        "            if feature not in cols_num:\n",
        "                # create label mapping for categorical feature values\n",
        "                mappings = {k: i for i, k in enumerate(df[feature])}\n",
        "                mapping[feature] = mappings\n",
        "                df[feature] = df[feature].map(mapping[feature])\n",
        "        for feature in cols_num: \n",
        "                try:\n",
        "                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n",
        "                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n",
        "                    if len(test_df.index) != 0:\n",
        "                        pipe = make_pipeline(StandardScaler(), model)\n",
        "\n",
        "                        y = np.log(train_df[feature]) # log-transform the data\n",
        "                        X_train = train_df.drop(feature, axis=1)\n",
        "                        test_df.drop(feature, axis=1, inplace=True)\n",
        "                        \n",
        "                        try:\n",
        "                            model = pipe.fit(X_train, y)\n",
        "                        except:\n",
        "                            y = train_df[feature] # use non-log-transformed data\n",
        "                            model = pipe.fit(X_train, y)\n",
        "                        if (y == train_df[feature]).all():\n",
        "                            pred = model.predict(test_df)\n",
        "                        else:\n",
        "                            pred = np.exp(model.predict(test_df)) # predict values\n",
        "\n",
        "                        test_df[feature]= pred\n",
        "\n",
        "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
        "                            # round back to INTs, if original data were INTs\n",
        "                            test_df[feature] = test_df[feature].round()\n",
        "                            test_df[feature] = test_df[feature].astype('Int64')\n",
        "                            df[feature].update(test_df[feature])                          \n",
        "                        else:\n",
        "                            df[feature].update(test_df[feature])  \n",
        "                        print('LINREG imputation succeeded', len(pred), feature)\n",
        "                except:\n",
        "                    print('LINREG imputation failed', feature)\n",
        "        for feature in df.columns: \n",
        "            try:   \n",
        "                # map categorical feature values back to original\n",
        "                mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
        "                df[feature] = df[feature].map(mappings_inv)\n",
        "            except:\n",
        "                pass\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def impute(df, imputer, type):\n",
        "        # function for imputing missing values in the data\n",
        "        cols_num = df.select_dtypes(include=np.number).columns \n",
        "\n",
        "        if type == 'num':\n",
        "            # numerical features\n",
        "            for feature in df.columns: \n",
        "                if feature in cols_num:\n",
        "                    if df[feature].isna().sum().sum() != 0:\n",
        "                        try:\n",
        "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)))\n",
        "                            counter = df[feature].isna().sum().sum() - df_imputed.isna().sum().sum()\n",
        "\n",
        "                            if (df[feature].fillna(-9999) % 1  == 0).all():\n",
        "                                df[feature] = df_imputed\n",
        "                                # round back to INTs, if original data were INTs\n",
        "                                df[feature] = df[feature].round()\n",
        "                                df[feature] = df[feature].astype('Int64')                                        \n",
        "                            else:\n",
        "                                df[feature] = df_imputed\n",
        "                            if counter != 0:\n",
        "                                print('imputation succeeded', str(counter, feature))\n",
        "                        except:\n",
        "                            print('imputation failed', str(feature))\n",
        "        else:\n",
        "            # categorical features\n",
        "            for feature in df.columns:\n",
        "                if feature not in cols_num:\n",
        "                    if df[feature].isna().sum()!= 0:\n",
        "                        try:\n",
        "                            mapping = dict()\n",
        "                            mappings = {k: i for i, k in enumerate(df[feature].dropna().unique(), 0)}\n",
        "                            mapping[feature] = mappings\n",
        "                            df[feature] = df[feature].map(mapping[feature])\n",
        "\n",
        "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)), columns=[feature])    \n",
        "                            counter = sum(1 for i, j in zip(list(df_imputed[feature]), list(df[feature])) if i != j)\n",
        "\n",
        "                            # round to integers before mapping back to original values\n",
        "                            df[feature] = df_imputed\n",
        "                            df[feature] = df[feature].round()\n",
        "                            df[feature] = df[feature].astype('Int64')  \n",
        "\n",
        "                            # map values back to original\n",
        "                            mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
        "                            df[feature] = df[feature].map(mappings_inv)\n",
        "                            if counter != 0:\n",
        "                                print('imputation succeeded', counter, feature)\n",
        "                        except:\n",
        "                            print('imputation failed', str( feature))\n",
        "        return df"
      ],
      "metadata": {
        "id": "T0EqpNWCKkvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# immpute values with linear regression\n",
        "lr = LinearRegression()\n",
        "df = lin_regression_impute(df, lr)"
      ],
      "metadata": {
        "id": "tkqgTMENKmq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# immpute values with KNN\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "df = impute(df, imputer, type='cat')"
      ],
      "metadata": {
        "id": "aNoshJiOKntU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = KNNImputer(n_neighbors=3)\n",
        "df = impute(df, imputer, type='num')"
      ],
      "metadata": {
        "id": "XdIPiq8AKvn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding\n",
        "def to_onehot(df, feature, limit=10):  \n",
        "        # function that encodes categorical features to OneHot encodings    \n",
        "        one_hot = pd.get_dummies(df[feature], prefix=feature)\n",
        "        if one_hot.shape[1] > limit:\n",
        "            print( feature, one_hot.shape[1])\n",
        "        # join the encoded df\n",
        "        df = df.join(one_hot)\n",
        "        return df\n",
        "\n",
        "def to_label(df, feature):\n",
        "        # function that encodes categorical features to label encodings \n",
        "        le = preprocessing.LabelEncoder()\n",
        "\n",
        "        df[feature] = le.fit_transform(df[feature].values)\n",
        "        mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
        "        \n",
        "        for key in mapping:\n",
        "            try:\n",
        "                if isnan(key):               \n",
        "                    replace = {mapping[key] : key }\n",
        "                    df[feature].replace(replace, inplace=True)\n",
        "            except:\n",
        "                pass\n",
        "        return df  \n",
        "\n",
        "\n",
        "cols_categ = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns) \n",
        "target_cols = cols_categ # encode ALL columns\n",
        "for feature in target_cols:\n",
        "  if df[feature].nunique() <=10:\n",
        "    df = to_onehot(df, feature)\n",
        "    print('Encoding to ONEHOT succeeded', feature)\n",
        "    df=df.drop([feature],axis=1)\n",
        "  elif df[feature].nunique() <=20:\n",
        "    df = to_label(df, feature)\n",
        "    print('Encoding to LABEL succeeded', feature)\n"
      ],
      "metadata": {
        "id": "5cDGoSs8KzhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelling"
      ],
      "metadata": {
        "id": "nJhpABhuK4pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler().fit(df.drop(\"Recurrence\",axis=1))\n"
      ],
      "metadata": {
        "id": "-3CDFz2SK6dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = scaler.transform(df.drop(\"Recurrence\",axis=1))\n",
        "y = df[\"Recurrence\"]"
      ],
      "metadata": {
        "id": "BTmMDUOPLD8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.20,random_state=0)"
      ],
      "metadata": {
        "id": "gtpmtKkbLFMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix,accuracy_score,confusion_matrix\n",
        "def Model(model):\n",
        "    global X,y,X_train, X_test, y_train, y_test\n",
        "    print(type(model).__name__)\n",
        "    pred = model.predict(X_test)\n",
        "    acs = accuracy_score(y_test,pred)\n",
        "    print(\"Accuracy Score             :\",acs)"
      ],
      "metadata": {
        "id": "ZO8pMrpRLGb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "def CrossValidationScore(model_list):\n",
        "    global X,y\n",
        "    \n",
        "    mean_cross_val_score = []\n",
        "    model_name           = []\n",
        "    \n",
        "    for model in model_list:\n",
        "        model_name.append(type(model).__name__)\n",
        "        \n",
        "    for i in model_list:\n",
        "        scores = cross_val_score(i, X, y, cv=5)\n",
        "        mean_cross_val_score.append(scores.mean())\n",
        "        \n",
        "    cvs = pd.DataFrame({\"Model Name\":model_name,\"CVS\":mean_cross_val_score})\n",
        "    return cvs.style.background_gradient(\"Greens\")"
      ],
      "metadata": {
        "id": "VxRp18BYLKxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "lwhtTT9VLOqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "dtc = DecisionTreeClassifier()\n",
        "lr = LogisticRegression()\n",
        "svm = SVC()\n",
        "knc = KNeighborsClassifier()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "abc = AdaBoostClassifier()\n",
        "\n",
        "keys = ['DecisionTreeClassifier','LogisticRegression','KNeighborsClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier','XGBClassifier']\n",
        "values = [DecisionTreeClassifier(), LogisticRegression(), KNeighborsClassifier(), RandomForestClassifier(), GradientBoostingClassifier(), AdaBoostClassifier(),XGBClassifier()]\n",
        "\n",
        "scores = []\n",
        "for value in values:\n",
        " Model(value)\n",
        "\n",
        "scores"
      ],
      "metadata": {
        "id": "goYhLzNqLRSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "model= RandomForestClassifier(n_estimators=100, criterion-'entropy', random_State=0)\n",
        "model.fit(X_train,y_train)\n",
        "predict=model.predict(X_test)\n",
        "print(classification_report(y_test,predict))\n",
        "print(mode.score(X_test,y_test))"
      ],
      "metadata": {
        "id": "CXM6Tch-L05e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compare models\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.barplot(x = scores, y = keys, palette='muted')\n",
        "plt.title(\"Model Scores\", fontsize=16, fontweight=\"bold\")"
      ],
      "metadata": {
        "id": "CK14AYVYL3J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dump pickle model file"
      ],
      "metadata": {
        "id": "-C_TQYOSOu6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model, open('rf_model.pkl', 'wb'))\n"
      ],
      "metadata": {
        "id": "gRKXrp32ML_w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}